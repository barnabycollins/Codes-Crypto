% !TEX TS-program = pdflatex
% !TEX encoding = UTF-8 Unicode

% This is a simple template for a LaTeX document using the "article" class.
% See "book", "report", "letter" for other types of document.

\documentclass[11pt]{article} % use larger type; default would be 10pt

\usepackage[utf8]{inputenc} % set input encoding (not needed with XeLaTeX)

%%% Examples of Article customizations
% These packages are optional, depending whether you want the features they provide.
% See the LaTeX Companion or other references for full information.

%%% PAGE DIMENSIONS
\usepackage[a4paper,left=2cm,right=2cm,top=2cm,bottom=2cm]{geometry}
%\usepackage{geometry} % to change the page dimensions
% \geometry{a4paper} % or letterpaper (US) or a5paper or....
% \geometry{margin=0in} % for example, change the margins to 2 inches all round
% \geometry{landscape} % set up the page for landscape
%   read geometry.pdf for detailed page layout information

\usepackage{graphicx} % support the \includegraphics command and options

\usepackage[parfill]{parskip} % Activate to begin paragraphs with an empty line rather than an indent

%%% PACKAGES
\usepackage{booktabs} % for much better looking tables
\usepackage{array} % for better arrays (eg matrices) in maths
\usepackage{paralist} % very flexible & customisable lists (eg. enumerate/itemize, etc.)
\usepackage{verbatim} % adds environment for commenting out blocks of text & for better verbatim
\usepackage{subfig} % make it possible to include more than one captioned figure/table in a single float
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{logicproof}
\usepackage{tikz}
\usetikzlibrary{arrows,petri,topaths}
\usepackage{float}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{textcomp}
\usepackage[T1]{fontenc}
% These packages are all incorporated in the memoir class to one degree or another...

%%% HEADERS & FOOTERS
\usepackage{fancyhdr} % This should be set AFTER setting up the page geometry
\pagestyle{fancy} % options: empty , plain , fancy
\renewcommand{\headrulewidth}{0pt} % customise the layout...
\lhead{}\chead{}\rhead{}
\lfoot{}\cfoot{\sffamily\thepage\normalfont}\rfoot{zrlr73}

%%% SECTION TITLE APPEARANCE
\usepackage{sectsty}
\allsectionsfont{\sffamily\mdseries\upshape} % (See the fntguide.pdf for font help)
% (This matches ConTeXt defaults)

%%% ToC (table of contents) APPEARANCE
\usepackage[nottoc,notlof,notlot]{tocbibind} % Put the bibliography in the ToC
\usepackage[titles,subfigure]{tocloft} % Alter the style of the Table of Contents
\renewcommand{\cftsecfont}{\rmfamily\mdseries\upshape}
\renewcommand{\cftsecpagefont}{\rmfamily\mdseries\upshape} % No bold!
\newcommand{\qedsymbol}{\rightline{$\blacksquare$}}
\newcommand{\textapprox}{\raisebox{0.5ex}{\texttildelow}}
\renewcommand{\familydefault}{\sfdefault}
%\renewcommand{\thesection}{Idea \arabic{section}:}

\usepackage[style=numeric-comp]{biblatex}

%%% END Article customizations

%%% The "real" document content comes below...

\title{\vspace{-1.6cm}Attacking DES on a what3words address}
\author{zrlr73}
\date{} % Activate to display a given date or no date (if empty),
         % otherwise the current date is printed 

\begin{document}
\maketitle



\section{Initial Thoughts \& Information}

As soon as I saw that ECB mode had been used, my first thought was of the fact that each 64-bit block is encrypted totally independently - that is, every 64 bits (or 8 characters) can be analysed completely separately. For a brute-force attack, this means that an attacker only needs to find the right 8-character string for each consecutive block and concatenate them, rather than guess the whole input in one go.

The provision of an exe file containing the encryption key did provide the opportunity to decompile that file, but I thought that would compromise the spirit of the coursework so decided to focus on other options first.

\subsection{My Computer}

The computer being used for this coursework was my desktop, which has an 8-core / 16-thread AMD Ryzen 7 1700 (overclocked to 3.7GHz), 16GB of RAM and plenty of free storage space. The single-core performance of the 1700 is very good, but its real strength is its multicore throughput.

I therefore planned to multithread any solutions I produced, since most cryptographic workloads are quite scalable when multithreading and breaking a (still fairly formidable) system like DES is a time-consuming task. However, I planned to only do this once I had it working on a single core and had found that it would return an answer in a reasonable timeframe, because there is no point putting work into multithreading something that will take centuries to complete!

\section{Initial attempts}

My first idea was to simply bruteforce every possible 64-bit string to see how long it would take: after a bit of experimentation I found that it would take time on the order of 34 years. Since this would mean submitting the coursework late, I decided that idea might be potentially a bit impractical. I also briefly researched more intelligent methods like differential cryptanalysis, but found that even with chosen plaintexts there would not be any particular performance improvement when compared with the advantage I had as a result of knowing the structure of the input.

\section{Dictionary attack}

\subsection{Initial attack}

The clear next step was to try a dictionary attack, generating possible what3words addresses from a \href{https://github.com/dwyl/english-words/blob/master/words_dictionary.json}{dictionary of \textapprox{}370,000 English words}. I initially started generating every possible what3words address before realising that this was a bit of a Bad Idea and would take an unreasonable quantity of time. I then had the realisation that I could probably limit the generated addresses to be exactly 16 characters long, since this was the length of the cipher text and no mention of padding had been made in the coursework brief. Combining this insight with the fact that what3words seemed to only use words comprising 4 or more characters didn't help much either, but it did mean that I was able to restrict the words to only those which were 4, 5 or 6 characters long. The memory usage also was quite unreasonable throughout this stage.

I then had the idea of generating first and second halves separately - this would remove a lot of redundancy because, for example, four different 16-character addresses could be summarised by five 8-character ones:

$$
\begin{matrix}
\verb|drops.rings.land| & \verb|drops.rinks.tall| & \verb|drops.ribs.desks| & \verb|drops.river.also|
\end{matrix}
$$
$$
\begin{matrix}
\verb|drops.ri| & \verb|ngs.land| & \verb|nks.tall| & \verb|bs.desks| & \verb|ver.also|
\end{matrix}
$$

saving 24 characters in this small-scale example. This meant that the set of items to check would be significantly smaller, especially at large scale, where thousands of full addresses would match to a single half. With this efficiency improvement in place it only took a couple of minutes to generate every possible half of a 16-character what3words address.

In this way I generated 36,098,043 first halves and 38,196,414 second halves. I then wrote code to use \verb|encrypt.exe| to try all first halves, and found that it would run in a timeframe that wasn't totally unreasonable, so used the Python \verb|multiprocessing| module to parallelise it on 16 threads. Following a series of runs of 50,000 first-halves I found that with this method I was able to test one half-address roughly every 0.027 seconds on average, putting me at roughly 550 hours (or around two weeks) to try every single possibility. Clearly this was not ideal, but I ran batches on the first half for a day or so, exhausting \textapprox{}1.8m first halves, until I had a new idea:

\subsection{Using a smaller dictionary}

I had noticed that the large dictionary contained an overwhelming number of obscure words like 'hokes' and 'reina', and done a small amount of research to see if there were any smaller word lists that contained every commonly-used word. I couldn't find any, but I had seen a \href{https://github.com/first20hours/google-10000-english/blob/master/google-10000-english-no-swears.txt}{list of the most frequent 10,000 English words} based on a dataset from Google. I thought that this was definitely worth a try, since using it would effectively prioritise the most likely set of words and I had already been automatically maintaining lists of halves that didn't match so time would not be wasted.

Running this gave me only 1,826,012 first halves and 1,695,629 second halves. Trying every first half after subtracting those that had already been tested took roughly 10 hours in total, and yielded the first half!

$$
\verb|tile.bil|
$$

I then adjusted the code to generate a list of second halves with the second word filtered to only include those that began with \verb|bil|. This gave me 4225 possible second halves, which were exhausted in 90 seconds with the answer:

$$
\verb|ls.print|
$$

If this had failed I would have done the same second-word reduction on the full 370,000-word dictionary, and would have considered using the \href{https://developer.what3words.com/tutorial/python}{what3words AutoSuggest API} to search for similar addresses (given that I already had \verb|tile.bil| to work from). I think that running through the reduction on the full dictionary wouldn't have taken longer than an hour or two.

\section{Result}

In summary, I found the following address:

$$
\verb|tile.bills.print|
$$

which I found to point to an unassuming car park in Philadelphia, USA. I must admit, this was initially a little underwhelming because I'd been expecting it to point to the top-secret location of a French nuclear bunker or a landmark or something. However, searching for the landscaping business at that location led me to stories I had seen before about Donald Trump's ill-fated mid-election press conference where Four Seasons Total Landscaping was booked as a venue instead of Four Seasons Hotel. Touché!


\end{document}
